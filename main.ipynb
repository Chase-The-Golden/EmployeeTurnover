{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality Check\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('HR_comma_sep.csv')\n",
    "\n",
    "print(\"SUM OF MISSING VALUES IN DATAFRAME\")\n",
    "print(df.isna().sum())\n",
    "num_df = df.drop(['sales', 'salary'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating factors of employee turnover\n",
    "\n",
    "# Correlation Matrix Heatmap\n",
    "sns.heatmap(num_df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Plot\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12,4), layout='constrained')\n",
    "\n",
    "# Employee Satisfaction\n",
    "ax0 = sns.histplot(df['satisfaction_level'], kde=True, ax=axs[0])\n",
    "ax0.set_title(\"EMPLOYEE SATISFACTION\")\n",
    "\n",
    "#  Employee Evaluation\n",
    "ax1 = sns.histplot(df['last_evaluation'], kde=True, ax=axs[1])\n",
    "ax1.set_title(\"EMPLOYEE EVALUATION\")\n",
    "\n",
    "# Employee Avg. Hours in a Month\n",
    "ax2 = sns.histplot(df['average_montly_hours'], kde=True, ax=axs[2])\n",
    "ax2.set_title(\"EMPLOYEE AVG MONTHLY HOURS\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employee Project Count & Leave Status\n",
    "sns.barplot(data=df, y='number_project', hue='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Correlation on employee turnover - i.e., employees who have left - is strongest in relation to satisfaction level, the time spent in the company, and whether the employee had suffered from a work accident.\n",
    "2. A concerningly high number of employees appear to have an overwhelmingly low satisfaction level, which will require further investigation in terms of turnover. \n",
    "3. There appears to be little correlation with the number of projects and their leave status, as evidenced by the correlation matrix & the barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering based off employee satisfaction & evaluation score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# DEBUG: Should print the columns satisfaction_level, last_evaluation, and left\n",
    "# print(df.iloc[:, [0, 1, 6]].head(10))\n",
    "X_kmeans = df.iloc[:, [0, 1, 6]]\n",
    "X_kmeans = X_kmeans[X_kmeans['left'] == 1]      # Filtering out employees who stayed\n",
    "model = KMeans(n_clusters=3, init=\"k-means++\")\n",
    "X_kmeans['y_kmeans'] = model.fit_predict(X_kmeans)\n",
    "\n",
    "plt.scatter(x=X_kmeans['satisfaction_level'], y=X_kmeans['last_evaluation'], c=X_kmeans['y_kmeans'])\n",
    "plt.xlabel('Employee Satisfaction')\n",
    "plt.ylabel('Last Evaluation')\n",
    "plt.show()\n",
    "\n",
    "print(X_kmeans['y_kmeans'].value_counts().sort_index())\n",
    "print(X_kmeans.groupby(['y_kmeans'])[['satisfaction_level', 'last_evaluation']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the clusters, most employees who have left have a low satisfaction & evaluation score (cluster 0) - self-explanatory.\n",
    "\n",
    "The other two clusters have similar high evaluation scores, but show great disparity in satisfaction levels.\n",
    "\n",
    "Cluster 1 shows a high satisfaction, this may be a retirement leave or perhaps a leave to a higher company.\n",
    "Cluster 2 shows a low satisfaction, this may be due to overworking or a stagnation in their role in the company despite adequate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling \"left\" class imbalance\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"# Employees Who Left\")\n",
    "print(\"0 - Not left\")\n",
    "print(\"1 - Left\")\n",
    "print(\"\\nBEFORE\")\n",
    "# Notice there is a large imbalance between those who have and haven't left\n",
    "print(df['left'].value_counts())\n",
    "\n",
    "# Preprocess categorical variables to numerical\n",
    "df_cat = df[['Work_accident', 'sales', 'salary']]\n",
    "df_cat = pd.get_dummies(df_cat, drop_first=True).astype(int)\n",
    "\n",
    "# Recombining the categorical & numerical variables\n",
    "X = df.drop(['left', 'Work_accident', 'sales', 'salary'], axis=1)\n",
    "X = pd.concat([X, df_cat], axis=1)\n",
    "y = df['left']\n",
    "\n",
    "# Splitting data before SMOTE?\n",
    "X_train_MAIN, X_test_MAIN, y_train_MAIN, y_test_MAIN = train_test_split(X, y, train_size=0.8, random_state=123)\n",
    "\n",
    "# SMOTE\n",
    "X_new, y_new = SMOTE().fit_resample(X_train_MAIN, y_train_MAIN)\n",
    "\n",
    "print(\"\\nAFTER\")\n",
    "print(y_new.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold cross validation & evaluation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# Model dictionary with tuple of model title & model itself.\n",
    "model_dict = {0 : (\"Logistic Regression\", LogisticRegression(solver=\"liblinear\")),\n",
    "              1 : (\"Random Forest Classifier\", RandomForestClassifier()),\n",
    "              2 : (\"Gradient Boosting Classifier\", GradientBoostingClassifier())}\n",
    "\n",
    "# List for future task of plotting\n",
    "proba_list = []\n",
    "y_list = []\n",
    "pred_list = []\n",
    "\n",
    "for i in range(0, 3):\n",
    "    model = model_dict[i][1]\n",
    "\n",
    "    print(model_dict[i][0])\n",
    "\n",
    "    # print(\"Accuracy Score:\")\n",
    "    for train_ind, test_ind in kf.split(X_new):\n",
    "        X_train, X_test = X_new.iloc[train_ind], X_new.iloc[test_ind]\n",
    "        y_train, y_test = y_new.iloc[train_ind], y_new.iloc[test_ind]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # DEBUG: Accuracy Score to track performance\n",
    "        # print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "        proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Appending values to respective lists\n",
    "    proba_list.append(proba[:, 1])  # Positives\n",
    "    y_list.append(y_test)\n",
    "    pred_list.append(y_pred)\n",
    "   \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC score & plot\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(16,4), layout='constrained')\n",
    "fig.suptitle(\"ROC Curve compilation for training models\", fontsize=18)\n",
    "fig.supylabel(\"True Pos\")\n",
    "\n",
    "# List for determining best model (highest AUC score in this case)\n",
    "auc_list = []\n",
    "\n",
    "for i in range(0, 3):\n",
    "    # Plotting roc curve of each model\n",
    "    fpr, tpr, thresholds = roc_curve(y_list[i], proba_list[i])\n",
    "    roc_auc = roc_auc_score(y_list[i], proba_list[i])\n",
    "\n",
    "    auc_list.append(roc_auc)\n",
    "\n",
    "    axs[i].set_title(model_dict[i][0])\n",
    "    axs[i].set_xlabel(\"False Pos\")\n",
    "    axs[i].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    axs[i].plot([0, 1], [0, 1], 'r--')\n",
    "    axs[i].legend(loc=\"lower right\", fontsize=\"x-large\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for i in range(0, 3):\n",
    "    print(f\"{model_dict[i][0]} Confusion Matrix\")\n",
    "    print(f\"{confusion_matrix(y_list[i], pred_list[i])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the context of evaluating employee turnovers, the precision metric should have higher priority since the employees retained who have been identified as a false positive can be remedied in their probability in leaving. False negatives - however, have already left and cannot be remedied to increase their likeliness to stay.\n",
    "\n",
    "Furthermore, the classification models - i.e., the last 2 models - show an oustanding amount of False Positives relative to False Negative errors, which may provide richer information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability prediction via best performing model\n",
    "\n",
    "ind = auc_list.index(max(auc_list))\n",
    "\n",
    "# DEBUG: Printing the index of the model with the highest AUC score\n",
    "# print(ind)\n",
    "\n",
    "# Receiving probability employee will leave\n",
    "proba_leave = model_dict[ind][1].predict_proba(X_test_MAIN)[:,[1]]\n",
    "\n",
    "# New dataframe for classifying employee leave status\n",
    "df_leave = pd.DataFrame(proba_leave, columns=['proba_leave'])\n",
    "\n",
    "# Classifying employees to respective bins, left-inclusive\n",
    "df_leave['leave_status'] = pd.cut(df_leave['proba_leave'], [0, .2, .6, .9, 1.01], labels=['G', 'Y', 'O', 'R'], right=False).astype('category')\n",
    "# print(df_leave.head())\n",
    "print(df_leave['leave_status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Green (G): Low probability to leave. No further action necessary.\n",
    "\n",
    "Yellow (Y): Likely to leave. Lighten workload by either decreasing the number of projects or average hours per month.\n",
    "\n",
    "Orange (O)): Very likely to leave. Consider offering promotion.\n",
    "\n",
    "Red (R): Leave is almost imminent. Either discuss new salary or no further action necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
